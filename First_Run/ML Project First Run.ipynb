{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cdf8c19-7ab1-46ee-a8fb-d211442ac4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Apple', 'Banana', 'Mixed', 'Orange']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.5726, Accuracy: 0.3542\n",
      "Epoch 2, Loss: 6.9277, Accuracy: 0.6875\n",
      "Epoch 3, Loss: 4.5538, Accuracy: 0.7750\n",
      "Epoch 4, Loss: 3.5774, Accuracy: 0.8542\n",
      "Epoch 5, Loss: 2.9568, Accuracy: 0.8833\n",
      "Epoch 6, Loss: 2.4214, Accuracy: 0.9083\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     95\u001b[39m images, labels = images.to(device), labels.to(device)  \u001b[38;5;66;03m# Move data to device\u001b[39;00m\n\u001b[32m     97\u001b[39m optimizer.zero_grad()          \u001b[38;5;66;03m# Clear previous gradients\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m     99\u001b[39m loss = criterion(outputs, labels)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m    100\u001b[39m loss.backward()                \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mFruitCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Import required libraries for data handling, model building, and visualization\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Define paths to your local training and testing image directories\n",
    "train_path = \"train\"\n",
    "test_path = \"test\"\n",
    "\n",
    "# Define image transformations for training data\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),            # Resize images to 100x100\n",
    "    transforms.RandomHorizontalFlip(),        # Randomly flip images horizontally for augmentation\n",
    "    transforms.RandomRotation(15),            # Randomly rotate images by up to ±15 degrees\n",
    "    transforms.ToTensor(),                    # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)  # Normalize pixel values to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Define transformations for test data (no augmentation)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),            # Resize images to 100x100\n",
    "    transforms.ToTensor(),                    # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)  # Normalize pixel values\n",
    "])\n",
    "\n",
    "# Load training and test datasets using folder structure\n",
    "train_data = ImageFolder(train_path, transform=transform_train)\n",
    "test_data = ImageFolder(test_path, transform=transform_test)\n",
    "\n",
    "# Create DataLoader to batch and shuffle the data\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print class labels detected from folder names\n",
    "print(\"Classes:\", train_data.classes)\n",
    "\n",
    "# Define the CNN architecture\n",
    "class FruitCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FruitCNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # First convolutional layer\n",
    "            nn.ReLU(),                                  # ReLU activation\n",
    "            nn.MaxPool2d(2),                             # Max pooling to reduce spatial size\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # Second convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), # Third convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),                                # Flatten the output to 1D\n",
    "            nn.Linear(128 * 12 * 12, 256),                # Fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),                              # Dropout for regularization\n",
    "            nn.Linear(256, 4)                             # Output layer for 4 fruit classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)  # Define forward pass\n",
    "\n",
    "# Use GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model, define loss function and optimizer\n",
    "model = FruitCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()                # Use cross entropy for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001\n",
    "\n",
    "# Lists to track training accuracy and loss\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 15\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to device\n",
    "\n",
    "        optimizer.zero_grad()          # Clear previous gradients\n",
    "        outputs = model(images)        # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()                # Backward pass\n",
    "        optimizer.step()               # Update weights\n",
    "\n",
    "        running_loss += loss.item()    # Accumulate loss\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "        total += labels.size(0)        # Total samples\n",
    "        correct += (predicted == labels).sum().item()  # Correct predictions\n",
    "\n",
    "    acc = correct / total             # Calculate accuracy\n",
    "    train_loss.append(running_loss)   # Record loss\n",
    "    train_acc.append(acc)             # Record accuracy\n",
    "\n",
    "    # Print metrics after each epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()  # Set model to evaluation mode\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# No need to calculate gradients during evaluation\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)  # Get predicted classes\n",
    "        y_pred.extend(preds.cpu().numpy())  # Store predictions\n",
    "        y_true.extend(labels.numpy())       # Store actual labels\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred, target_names=train_data.classes))\n",
    "\n",
    "# Plot training accuracy over epochs\n",
    "plt.plot(train_acc, label='Train Accuracy')\n",
    "plt.title('Training Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training loss over epochs\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f4645-5435-421d-9254-89260df7eedf",
   "metadata": {},
   "source": [
    "# Code Rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3723c7-a50e-486a-b772-fd148bfd78f4",
   "metadata": {},
   "source": [
    "| Component       | Purpose                                   |\n",
    "| --------------- | ----------------------------------------- |\n",
    "| 15 Epochs       | Enough for small dataset; avoids overfit  |\n",
    "| Resize(100x100) | Standardizes input size                   |\n",
    "| Flip + Rotate   | Augments data for better generalization   |\n",
    "| Normalize       | Speeds up and stabilizes learning         |\n",
    "| CNN Layers      | Extract low-to-high level visual features |\n",
    "| Dropout         | Regularization to reduce overfitting      |\n",
    "| Fully Connected | Decision making for classification        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c19098-5169-425f-9f2d-ef796f16bdf5",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation Summary\n",
    "\n",
    "---\n",
    "\n",
    "## Model Training Observations\n",
    "\n",
    "- **Epochs**: 15  \n",
    "- **Initial Accuracy**: 45.83%  \n",
    "- **Final Training Accuracy**: 97.92%  \n",
    "- **Training Loss**: Decreased from 9.78 to approximately 0.45\n",
    "\n",
    "### Trend:\n",
    "- The model shows steady learning and convergence.\n",
    "- Accuracy and loss improvements indicate effective training and model fit.\n",
    "\n",
    "---\n",
    "\n",
    "## Test Set Performance\n",
    "\n",
    "| Class   | Precision | Recall | F1-score | Support |\n",
    "|---------|-----------|--------|----------|---------|\n",
    "| Apple   | 0.90      | 1.00   | 0.95     | 19      |\n",
    "| Banana  | 0.80      | 0.89   | 0.84     | 18      |\n",
    "| Mixed   | 0.00      | 0.00   | 0.00     | 5       |\n",
    "| Orange  | 0.89      | 0.94   | 0.92     | 18      |\n",
    "\n",
    "- Apple and Orange were classified very well.\n",
    "- Mixed class was completely misclassified — the model made no correct predictions.\n",
    "- Overall test accuracy was 87%.\n",
    "\n",
    "---\n",
    "\n",
    "## Warnings and Issues\n",
    "\n",
    "### PIL Warning:\n",
    "`Palette images with Transparency expressed in bytes should be converted to RGBA images`  \n",
    "Some images (e.g., `.png` or `.gif`) contain transparency and should be explicitly converted to RGBA to ensure proper processing.\n",
    "\n",
    "### UndefinedMetricWarning from sklearn:\n",
    "`Precision is ill-defined and being set to 0.0 in labels with no predicted samples.`  \n",
    "This occurs because the model never predicted the Mixed class, leading to undefined precision and recall values for that class.\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Performance Summary\n",
    "\n",
    "- **Final Test Accuracy**: 87%\n",
    "- **Macro Average F1-score**: 0.68 (lower due to poor performance on Mixed)\n",
    "- **Weighted Average F1-score**: 0.83 (heavily influenced by Apple and Orange)\n",
    "\n",
    "---\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. Check class distribution in the training set to ensure the Mixed class is not underrepresented.\n",
    "2. Add more training examples for the Mixed class or apply data augmentation.\n",
    "3. Consider using class weighting in the loss function to compensate for class imbalance.\n",
    "4. Plot a confusion matrix to understand where the model is confusing Mixed with other classes.\n",
    "5. Ensure all images are correctly formatted and converted to RGB or RGBA where necessary.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a3365-388d-4543-967c-be9ad712d0e8",
   "metadata": {},
   "source": [
    "# Why a 3-Layer CNN Architecture Was Chosen\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Progressive Feature Extraction\n",
    "\n",
    "- **Layer 1** learns basic features such as edges and textures.\n",
    "- **Layer 2** identifies more complex patterns like shapes and contours.\n",
    "- **Layer 3** extracts high-level, abstract features (e.g., outlines or combinations of shapes).\n",
    "- This hierarchy allows the model to understand images from simple to complex representations.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Suitable for Simple Visual Categories\n",
    "\n",
    "- The dataset involves fruits, which have **distinct colors, textures, and shapes**.\n",
    "- The resized image dimension is **100×100**, which is relatively low.\n",
    "- A deeper architecture would be overkill and may introduce unnecessary complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Balanced Depth to Prevent Overfitting\n",
    "\n",
    "- **Too shallow (1–2 layers)**: May underfit and miss important patterns.\n",
    "- **Too deep (5+ layers)**: May overfit or require more data and compute.\n",
    "- **3 layers** is a balanced choice, offering enough capacity to learn without overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Efficient Feature Map Reduction\n",
    "\n",
    "- Input size: **100×100**\n",
    "- After 3 `MaxPool2d(2)` layers:\n",
    "  - Output size reduces as follows: `100 → 50 → 25 → 12`\n",
    "- The final feature maps are small and efficient to flatten for fully connected layers.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Proven Practical Effectiveness\n",
    "\n",
    "- 3-layer CNNs perform well on small to medium image datasets (e.g., MNIST, CIFAR-10).\n",
    "- Ideal for classification tasks with a **limited number of classes**.\n",
    "- Fast to train, interpretable, and good for prototyping or educational use.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Reason                            | Explanation                                                                 |\n",
    "|-----------------------------------|-----------------------------------------------------------------------------|\n",
    "| Hierarchical feature learning     | Captures visual patterns from edges to object shapes                        |\n",
    "| Appropriate model depth           | Deep enough to learn, but avoids unnecessary complexity                     |\n",
    "| Reduces overfitting risk          | Suitable depth for datasets with limited samples per class                  |\n",
    "| Efficient for 100×100 images      | Spatial dimensions reduce nicely through pooling                            |\n",
    "| Fast and effective                | Trains quickly, works well for fruit classification tasks                   |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04033c16-de97-42ef-9f75-9d3ed8bde891",
   "metadata": {},
   "source": [
    "# Recommended Number of Training Images and Rationale\n",
    "\n",
    "---\n",
    "\n",
    "## Recommended Number of Images per Class\n",
    "\n",
    "| Class     | Minimum Recommended | Ideal Target | Rationale |\n",
    "|-----------|---------------------|--------------|-----------|\n",
    "| Apple     | ≥ 100               | 200–500      | Performs well; more data helps improve generalization. |\n",
    "| Banana    | ≥ 100               | 200–500      | Decent performance; more examples improve robustness. |\n",
    "| Mixed     | ≥ 200               | 300–600+     | Currently underperforms; needs significantly more data. |\n",
    "| Orange    | ≥ 100               | 200–500      | Strong baseline; should maintain class balance. |\n",
    "\n",
    "---\n",
    "\n",
    "## Justifications and Rationale\n",
    "\n",
    "### 1. Preventing Class Imbalance\n",
    "\n",
    "- The `Mixed` class fails due to likely underrepresentation.\n",
    "- Adding more examples ensures balanced training and fairer model attention.\n",
    "- Balanced datasets reduce bias and improve classification accuracy across all classes.\n",
    "\n",
    "### 2. Enhancing Generalization\n",
    "\n",
    "- CNNs require visual variety (angle, lighting, background) to generalize.\n",
    "- Small datasets (<100/class) often cause overfitting — the model memorizes instead of learning patterns.\n",
    "- 300–500 images per class offer enough variability for a simple CNN to generalize well.\n",
    "\n",
    "### 3. Data vs Model Complexity\n",
    "\n",
    "- Your model is a **3-layer CNN**, which is relatively simple and data-efficient.\n",
    "- Such models typically perform well with 200–500 images per class, especially when combined with data augmentation.\n",
    "\n",
    "### 4. Empirical Evidence\n",
    "\n",
    "- Datasets like CIFAR-10 and Flowers102 use ~500+ images/class for good performance.\n",
    "- Deeper models like ResNet often need more data, but shallower models benefit greatly from just 300–600/class.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Recommendation\n",
    "\n",
    "| Class Type           | Minimum (per class) | Ideal (per class) | Priority     |\n",
    "|----------------------|----------------------|-------------------|--------------|\n",
    "| Well-performing      | 100–150              | 300–500           | Medium       |\n",
    "| Mid-performing       | 100–200              | 300–500           | Medium       |\n",
    "| Underperforming      | 200–300              | 400–600+          | High (focus) |\n",
    "\n",
    "> Aim for **~1500–2000 total images**, with **additional focus on the 'Mixed' class**.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Step\n",
    "\n",
    "Consider using data augmentation or collecting more labeled images. This will enhance the model’s ability to generalize and improve its accuracy across all classes.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb0c1a-681b-43f2-8128-6906e0d26869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
